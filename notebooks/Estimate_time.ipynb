{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b465ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import librosa\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.layers import Layer, Conv1D, Softmax\n",
    "from keras.saving import register_keras_serializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c3464857",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_PATH = \"../data/Crema_Data/1001_ITH_SAD_XX.wav\"\n",
    "TARGET_SR = 16000   \n",
    "MAX_SAMPLES = 48000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a059d679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_wavenet(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=TARGET_SR)\n",
    "\n",
    "    y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "    if len(y) > MAX_SAMPLES:\n",
    "        y = y[:MAX_SAMPLES]\n",
    "    else:\n",
    "        y = np.pad(y, (0, MAX_SAMPLES - len(y)), mode='constant')\n",
    "\n",
    "    return y.reshape(1, MAX_SAMPLES, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bbf5c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1Ô∏è‚É£ ƒê·ªãnh Nghƒ©a Custom Layer Cho WaveNet =====\n",
    "@register_keras_serializable()\n",
    "class AttentionPooling(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.score_conv = Conv1D(1, 1, padding='same', name=\"attn_score_conv\")\n",
    "        self.softmax    = Softmax(axis=1, name=\"attn_weights\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        score   = self.score_conv(inputs)\n",
    "        weights = self.softmax(score)\n",
    "        return tf.reduce_sum(weights * inputs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "77c7127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_vgg16(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=TARGET_SR)\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    # Resize v·ªÅ ƒë√∫ng input VGG16 (gi·∫£ ƒë·ªãnh 224x224x3)\n",
    "    S_resized = tf.image.resize(S_dB[..., np.newaxis], (224,224)).numpy()\n",
    "    S_rgb = np.repeat(S_resized, 3, axis=-1)  # Chuy·ªÉn grayscale -> RGB\n",
    "    return np.expand_dims(S_rgb, axis=0)  # (batch, height, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "343ec363",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = {\n",
    "    \"WaveNet\": {\n",
    "        \"path\": \"wavenet_ser_model.keras\",\n",
    "        \"preprocess\": preprocess_for_wavenet\n",
    "    },\n",
    "    \"VGG16\": {\n",
    "        \"path\": \"vgg16_model.keras\",\n",
    "        \"preprocess\": preprocess_for_vgg16\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5a23d8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ Predict v·ªõi m√¥ h√¨nh: WaveNet\n",
      "‚û°Ô∏è Th·ªùi gian predict: 1.3268 gi√¢y | D·ª± ƒëo√°n l·ªõp: 3\n",
      "\n",
      "‚è≥ Predict v·ªõi m√¥ h√¨nh: VGG16\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional_6\" is incompatible with the layer: expected shape=(None, 128, 180, 3), found shape=(1, 224, 224, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# ƒêo th·ªùi gian\u001b[39;00m\n\u001b[32m     20\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m pred = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m end_time = time.time()\n\u001b[32m     24\u001b[39m predict_time = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/1203_ISEmotalk/tf/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/1203_ISEmotalk/tf/lib/python3.12/site-packages/keras/src/layers/input_spec.py:245\u001b[39m, in \u001b[36massert_input_compatibility\u001b[39m\u001b[34m(input_spec, inputs, layer_name)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    244\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim != dim:\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    246\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of layer \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m is \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    247\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mincompatible with the layer: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    249\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    250\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: Input 0 of layer \"functional_6\" is incompatible with the layer: expected shape=(None, 128, 180, 3), found shape=(1, 224, 224, 3)"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for name, config in model_files.items():\n",
    "    print(f\"\\n‚è≥ Predict v·ªõi m√¥ h√¨nh: {name}\")\n",
    "\n",
    "    # Load Model\n",
    "    if name == \"WaveNet\":\n",
    "        model = tf.keras.models.load_model(\n",
    "            config[\"path\"],\n",
    "            custom_objects={\"AttentionPooling\": AttentionPooling},\n",
    "            compile=False\n",
    "        )\n",
    "    else:  # VGG16\n",
    "        model = tf.keras.models.load_model(config[\"path\"], compile=False)\n",
    "\n",
    "    # X·ª≠ l√Ω input\n",
    "    audio_input = config[\"preprocess\"](AUDIO_PATH)\n",
    "\n",
    "    # ƒêo th·ªùi gian\n",
    "    start_time = time.time()\n",
    "    pred = model.predict(audio_input, verbose=0)\n",
    "    end_time = time.time()\n",
    "\n",
    "    predict_time = end_time - start_time\n",
    "    predicted_class = np.argmax(pred)\n",
    "\n",
    "    print(f\"‚û°Ô∏è Th·ªùi gian predict: {predict_time:.4f} gi√¢y | D·ª± ƒëo√°n l·ªõp: {predicted_class}\")\n",
    "    results[name] = predict_time\n",
    "\n",
    "# ===== 6Ô∏è‚É£ T·ªïng K·∫øt =====\n",
    "print(\"\\nüéØ T·ªïng K·∫øt Th·ªùi Gian D·ª± ƒêo√°n:\")\n",
    "for model_name, t in results.items():\n",
    "    print(f\"{model_name}: {t:.4f} gi√¢y\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
